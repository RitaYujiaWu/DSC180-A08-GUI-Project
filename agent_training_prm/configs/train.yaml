seed: 4321

paths:
  osworld_root: "./third_party/OSWorld"
  test_config_base_dir: "./third_party/OSWorld/evaluation_examples/examples"
  # This is OSWorld-style "meta" file: domain -> list[uuid]   [oai_citation:6â€¡GitHub](https://github.com/xlang-ai/OSWorld/issues/147)
  test_all_meta_path: "./third_party/OSWorld/evaluation_examples/test_small.json"

env:
  provider_name: "docker"     # or "vmware", depending on your setup
  os_type: "Ubuntu"           # "Windows" if running windows tasks
  path_to_vm: null            # set if using vmware; for docker often None
  headless: true
  action_space: "pyautogui"
  observation_type: "screenshot"  # affects require_a11y_tree
  screen_width: 1920
  screen_height: 1080
  max_steps: 1
  post_action_sleep_s: 3.0
  cdp_wait_timeout_s: 45
  cdp_wait_interval_s: 1.0

prm:
  base_url: "http://ds-serv11.ucsd.edu:8003/v1"
  model: "qwen3vl-prm"        # served-model-name on your vLLM server
  timeout_s: 30
  max_retries: 2
  window_steps: 3             # <<< TUNABLE PRM HISTORY WINDOW K
  reward_scale: 1.0
  reward_clip: [0.0, 1.0]

  debug: false
  debug_every: 1          # log every step (set to 5/10 to reduce spam)
  debug_dir: "./runs/prm_debug"
  return_rationale: true  # ask prm to include short explanation
  rationale_max_chars: 300

policy:
  model_name_or_path: "Qwen/Qwen2.5-VL-3B-Instruct"
  device: "cuda"
  max_new_tokens: 256
  temperature: 0.7
  top_p: 0.9

train:
  total_episodes: 1
  gamma: 0.99
  gae_lambda: 0.95
  lr: 1.0e-5
  clip_ratio: 0.2
  vf_coef: 0.5
  ent_coef: 0.01
  train_epochs: 1
  minibatch_size: 1
  grad_clip_norm: 1.0

logging:
  out_dir: "./runs/osworld_prm_ppo"
  log_every: 1
  save_every: 1