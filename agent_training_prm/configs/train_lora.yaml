seed: 4321

paths:
  osworld_root: "./third_party/OSWorld"
  test_config_base_dir: "./third_party/OSWorld/evaluation_examples/examples"
  test_all_meta_path: "./third_party/OSWorld/evaluation_examples/train.json"

env:
  provider_name: "docker"
  os_type: "Ubuntu"
  path_to_vm: null
  headless: true
  action_space: "pyautogui"
  observation_type: "screenshot"
  screen_width: 1920
  screen_height: 1080
  max_steps: 4
  post_action_sleep_s: 3.0
  cdp_wait_timeout_s: 45
  cdp_wait_interval_s: 1.0

prm:
  base_url: "http://ds-serv11.ucsd.edu:8003/v1"
  model: "qwen3vl-prm"
  timeout_s: 30
  max_retries: 2
  window_steps: 3
  reward_scale: 1.0
  reward_clip: [0.0, 1.0]

  debug: false
  debug_every: 1
  debug_dir: "./runs/prm_debug"
  return_rationale: true
  rationale_max_chars: 300

policy:
  model_name_or_path: "Qwen/Qwen3-VL-4B-Instruct"
  device: "cuda"
  max_new_tokens: 128
  temperature: 0.7
  top_p: 0.9
  # Text-only cap (usually not needed if you use max_total_tokens). Set null to disable.
  max_input_tokens: null
  # Keep some room so PPO can append the action without it being truncated.
  action_token_reserve: 40
  # Truncate on the left to preserve the assistant/action suffix.
  truncation_side: "left"
  # Optional: cap total tokens (text + vision tokens). Set to null to disable.
  max_total_tokens: 9500
  # When enforcing max_total_tokens, always keep at least this many text tokens.
  min_text_tokens: 256
  # Reduces activation memory during PPO updates.
  gradient_checkpointing: true
  history_n: 2
  # Downscale screenshot before feeding the VLM if pixels exceed this budget.
  # 1920x1080 ~= 2.07M pixels, so the old default (1600*1600=2.56M) did NOT downscale.
  # Try 1024*1024 (=1.05M) to cut vision tokens/memory, at some accuracy cost.
  max_image_pixels: 700000

  lora:
    enabled: true
    r: 16
    alpha: 32
    dropout: 0.05
    bias: "none"
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    freeze_vision_encoder: true
    vision_param_keywords: ["vision", "visual", "vision_tower", "image_tower", "vit"]

train:
  # In this codebase, 1 episode selects 1 task config.
  # 1 "epoch" over train.json == 330 episodes.
  # If true, each GPU/rank runs its own OSWorld env + PRM for faster multi-GPU.
  # Note: one PPO update happens per episode *per rank*.
  rollout_per_rank: true
  total_episodes: 990  # 3 epochs over train.json 
  gamma: 0.99
  gae_lambda: 0.95
  lr: 1.0e-5
  clip_ratio: 0.2
  vf_coef: 0.5
  ent_coef: 0.01
  train_epochs: 1
  minibatch_size: 1
  grad_clip_norm: 1.0

logging:
  out_dir: "./runs/osworld_prm_ppo_lora_epoch1_2"
  log_every: 1
  # Write per-step screenshots under ${out_dir}/screenshots/epXXXX/.
  # Disable to reduce disk I/O.
  save_screenshots: false
  # Save ckpt once per epoch (one full pass over paths.test_all_meta_path).
  save_every: 110

  # Optional Weights & Biases logging.
  # Enable with: wandb.enabled: true
  wandb:
    enabled: true
    project: "osworld_prm_ppo"
    entity: null
    name: osworld_prm_ppo_lora_4steps_epoch1_2
    group: null
    tags: []
    mode: "online"  # or: "offline"
    upload_checkpoints: false  # set true to upload model checkpoints as W&B artifacts (can be large, so false by default)
    upload_logs: false  # set true to upload training logs as W&B artifacts

# validation evaluation during training.
eval:
  enabled: false
  meta_path: "./third_party/OSWorld/evaluation_examples/test_small.json"
  every: epoch
  max_episodes: null  # test_small size today; set null to run all
  do_sample: false   # deterministic (greedy-ish) decoding
  log_table: true
